{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:10:03.220788Z",
     "start_time": "2025-04-16T17:09:58.143074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))"
   ],
   "id": "c80547e0598261e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.6.0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 4.1 Number of parameters in feed forward and attention modules\n",
    "Calculate and compare the number of parameters that are contained in the feed for- ward module and those that are contained in the multi-head attention module."
   ],
   "id": "49c20eee5347b346"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:10:04.973937Z",
     "start_time": "2025-04-16T17:10:04.962563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ],
   "id": "25a5b4d14cbde1a6",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:10:06.607821Z",
     "start_time": "2025-04-16T17:10:06.603844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ],
   "id": "9f80438d5f5c835f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:10:08.277042Z",
     "start_time": "2025-04-16T17:10:08.273663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ],
   "id": "6ecc069220092ef4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:10:09.992414Z",
     "start_time": "2025-04-16T17:10:09.987595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ],
   "id": "2ea95c1b5a51f643",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:10:12.072743Z",
     "start_time": "2025-04-16T17:10:11.925200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from previous_chapters import MultiHeadAttention\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ],
   "id": "7a876dda6b5d3751",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T01:17:46.872982Z",
     "start_time": "2025-04-16T01:17:46.789523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "print(block)"
   ],
   "id": "31164136b50913f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerBlock(\n",
      "  (att): MultiHeadAttention(\n",
      "    (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ff): FeedForward(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (1): GELU()\n",
      "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm()\n",
      "  (norm2): LayerNorm()\n",
      "  (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T01:19:44.368897Z",
     "start_time": "2025-04-16T01:19:44.350587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_parameters = sum(p.numel() for p in block.ff.parameters())\n",
    "print(\"Total feed forward parameters: \", total_parameters)"
   ],
   "id": "52c97d8480a551c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total feed forward parameters:  4722432\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T01:19:45.645143Z",
     "start_time": "2025-04-16T01:19:45.641040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_parameters = sum(p.numel() for p in block.att.parameters())\n",
    "print(\"Total attention module parameters: \", total_parameters)"
   ],
   "id": "a24fbee2d87d7325",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total attention module parameters:  2360064\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 4.2 Initializing larger GPT models\n",
    "We initialized a 124-million-parameter GPT model, which is known as “GPT-2 small.” Without making any code modifications besides updating the configuration file, use the GPTModel class to implement GPT-2 medium (using 1,024-dimensional embed- dings, 24 transformer blocks, 16 multi-head attention heads), GPT-2 large (1,280- dimensional embeddings, 36 transformer blocks, 20 multi-head attention heads), and GPT-2 XL (1,600-dimensional embeddings, 48 transformer blocks, 25 multi-head attention heads). As a bonus, calculate the total number of parameters in each GPT model."
   ],
   "id": "e7a35d8aa5806260"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:10:19.475504Z",
     "start_time": "2025-04-16T17:10:19.468332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ],
   "id": "2cc8a7628186746f",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:10:28.602729Z",
     "start_time": "2025-04-16T17:10:28.597139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_config(config_name):\n",
    "    if config_name == \"Small\":\n",
    "        config = {\n",
    "            \"vocab_size\": 50257,    # Vocabulary size\n",
    "            \"context_length\": 1024, # Context length\n",
    "            \"emb_dim\": 768,         # Embedding dimension\n",
    "            \"n_heads\": 12,          # Number of attention heads\n",
    "            \"n_layers\": 12,         # Number of layers\n",
    "            \"drop_rate\": 0.1,       # Dropout rate\n",
    "            \"qkv_bias\": False       # Query-Key-Value bias\n",
    "        }\n",
    "    elif config_name == \"Medium\":\n",
    "        config = {\n",
    "            \"vocab_size\": 50257,    # Vocabulary size\n",
    "            \"context_length\": 1024, # Context length\n",
    "            \"emb_dim\": 1024,         # Embedding dimension\n",
    "            \"n_heads\": 16,          # Number of attention heads\n",
    "            \"n_layers\": 24,         # Number of layers\n",
    "            \"drop_rate\": 0.1,       # Dropout rate\n",
    "            \"qkv_bias\": False       # Query-Key-Value bias\n",
    "        }\n",
    "    elif config_name == \"Large\":\n",
    "        config = {\n",
    "            \"vocab_size\": 50257,    # Vocabulary size\n",
    "            \"context_length\": 1024, # Context length\n",
    "            \"emb_dim\": 1280,         # Embedding dimension\n",
    "            \"n_heads\": 20,          # Number of attention heads\n",
    "            \"n_layers\": 36,         # Number of layers\n",
    "            \"drop_rate\": 0.1,       # Dropout rate\n",
    "            \"qkv_bias\": False       # Query-Key-Value bias\n",
    "        }\n",
    "    elif config_name == \"XL\":\n",
    "        config = {\n",
    "            \"vocab_size\": 50257,    # Vocabulary size\n",
    "            \"context_length\": 1024, # Context length\n",
    "            \"emb_dim\": 1600,         # Embedding dimension\n",
    "            \"n_heads\": 25,          # Number of attention heads\n",
    "            \"n_layers\": 48,         # Number of layers\n",
    "            \"drop_rate\": 0.1,       # Dropout rate\n",
    "            \"qkv_bias\": False       # Query-Key-Value bias\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid configuration size: \", config_name)\n",
    "\n",
    "    return config"
   ],
   "id": "e037c8140b7df894",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:10:33.018123Z",
     "start_time": "2025-04-16T17:10:33.012480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_size(model): # based on chapter code\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total number of parameters: {total_params:,}\")\n",
    "\n",
    "    total_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "    print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")\n",
    "\n",
    "    # Calculate the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "    total_size_bytes = total_params * 4\n",
    "\n",
    "    # Convert to megabytes\n",
    "    total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "    print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ],
   "id": "69d1888cb267a256",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T16:32:05.026212Z",
     "start_time": "2025-04-16T16:31:32.044309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for config_name in (\"Small\", \"Medium\", \"Large\", \"XL\"):\n",
    "    print(\"GPT-2 \", config_name, \":\")\n",
    "    model = GPTModel(get_config(config_name))\n",
    "    calculate_size(model)\n",
    "\n",
    "#Can't run this on my laptop without it crashing"
   ],
   "id": "d7fc65b4c27e739d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2  Small :\n",
      "GPT-2  Medium :\n",
      "GPT-2  Large :\n",
      "GPT-2  XL :\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 4.3 Using separate dropout parameters\n",
    "At the beginning of this chapter, we defined a global drop_rate setting in the GPT_ CONFIG_124M dictionary to set the dropout rate in various places throughout the GPTModel architecture. Change the code to specify a separate dropout value for the various dropout layers throughout the model architecture. (Hint: there are three dis- tinct places where we used dropout layers: the embedding layer, shortcut layer, and multi-head attention module.)"
   ],
   "id": "9d6084701f767a04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:10:36.572253Z",
     "start_time": "2025-04-16T17:10:36.568956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = {\n",
    "            \"vocab_size\": 50257,    # Vocabulary size\n",
    "            \"context_length\": 1024, # Context length\n",
    "            \"emb_dim\": 1024,         # Embedding dimension\n",
    "            \"n_heads\": 16,          # Number of attention heads\n",
    "            \"n_layers\": 24,         # Number of layers\n",
    "            \"embedding_drop_rate\": 0.1,       # EMBEDDING Dropout rate\n",
    "            \"shortcut_drop_rate\": 0.2,        # SHORTCUT Dropout rate\n",
    "            \"attention_drop_rate\": 0.3,      # MULTI-HEAD ATTENTION Dropout rate\n",
    "            \"qkv_bias\": False       # Query-Key-Value bias\n",
    "        }"
   ],
   "id": "57d7ee94c4bd252a",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:10:38.379375Z",
     "start_time": "2025-04-16T17:10:38.374732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"attention_drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"shortcut_drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ],
   "id": "e73f77b38f4be0a7",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:11:27.177063Z",
     "start_time": "2025-04-16T17:11:27.168766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"embedding_drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ],
   "id": "7b02faea8976c524",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:51:53.575116Z",
     "start_time": "2025-04-16T17:51:50.313921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(config)"
   ],
   "id": "ae8cb1b4c39bf342",
   "outputs": [],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
