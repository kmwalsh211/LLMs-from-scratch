{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 2.1 - Byte pair encoding of unknown words\n",
    "Try the BPE tokenizer from the tiktoken library on the unknown words “Akwirw ier” and print the individual token IDs. Then, call the decode function on each of the resulting integers in this list to reproduce the mapping shown in figure 2.11. Lastly, call the decode method on the token IDs to check whether it can reconstruct the original input, “Akwirw ier.”"
   ],
   "id": "332f9f4327f581da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T03:06:22.140650Z",
     "start_time": "2025-03-16T03:06:21.740262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "unknown_text = (\"Akwirw ier\")"
   ],
   "id": "a01ea0e090531620",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T03:06:26.020440Z",
     "start_time": "2025-03-16T03:06:26.011704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_ids = tokenizer.encode(unknown_text)\n",
    "print(\"Individual token IDs: \", token_ids)"
   ],
   "id": "267e743747a94aab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual token IDs:  [33901, 86, 343, 86, 220, 959]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T03:06:28.998262Z",
     "start_time": "2025-03-16T03:06:28.989696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in token_ids:\n",
    "    print(i, ' ---> ', tokenizer.decode([i]))"
   ],
   "id": "6f2ea585bfcd5ab1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33901  --->  Ak\n",
      "86  --->  w\n",
      "343  --->  ir\n",
      "86  --->  w\n",
      "220  --->   \n",
      "959  --->  ier\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T03:06:31.127543Z",
     "start_time": "2025-03-16T03:06:31.121755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reconstructed_text = tokenizer.decode(token_ids)\n",
    "print(\"Reconstructed text through decode method: \", reconstructed_text)"
   ],
   "id": "578283d14a8b1d17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed text through decode method:  Akwirw ier\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 2.2 - Data loaders with different strides and context sizes\n",
    "To develop more intuition for how the data loader works, try to run it with different settings such as max_length=2 and stride=2, and max_length=8 and stride=2."
   ],
   "id": "f848759753457433"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T03:08:16.389821Z",
     "start_time": "2025-03-16T03:08:16.381372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ],
   "id": "6c9f6cc87d37d80e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T03:08:19.302641Z",
     "start_time": "2025-03-16T03:08:19.296354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ],
   "id": "27f0a83336ab91f0",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T03:08:20.941982Z",
     "start_time": "2025-03-16T03:08:20.935962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ],
   "id": "8cd91ff34fae2385",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### With max_length = 2, stride = 2",
   "id": "feb99808355f2ec2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T03:18:34.501204Z",
     "start_time": "2025-03-16T03:18:34.444184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=2, max_length=2, stride=2, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ],
   "id": "12b804cdc56e439b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367],\n",
      "        [2885, 1464]]), tensor([[ 367, 2885],\n",
      "        [1464, 1807]])]\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T03:18:36.425402Z",
     "start_time": "2025-03-16T03:18:36.416146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ],
   "id": "c329092ae77a6f26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1807, 3619],\n",
      "        [ 402,  271]]), tensor([[ 3619,   402],\n",
      "        [  271, 10899]])]\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### With max_length = 8, stride = 2",
   "id": "602a6d257aa24bdb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T03:19:58.463645Z",
     "start_time": "2025-03-16T03:19:58.412205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=8, stride=2, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ],
   "id": "afa3195dc9e7b10b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464, 1807, 3619,  402,  271]]), tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899]])]\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T03:19:48.435234Z",
     "start_time": "2025-03-16T03:19:48.423870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ],
   "id": "6949e27ee61db6b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138]]), tensor([[ 1464,  1807,  3619,   402,   271, 10899,  2138,   257]])]\n"
     ]
    }
   ],
   "execution_count": 37
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
